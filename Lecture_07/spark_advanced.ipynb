{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Spark\n",
    "**Andrey Titov**  \n",
    "andrey.titov@bigdatateam.org  \n",
    "Big Data Instructor @ BigData Team  \n",
    "https://bigdatateam.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Партиционирование\n",
    "+ Планы выполнения задач\n",
    "+ Оптимизация соединений и группировок\n",
    "+ Управление схемой данных\n",
    "+ Оптимизатор запросов Catalyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Партиционирование\n",
    "RDD и DF являются представляют собой классы, описывающие распределенные коллекции данных. Они (коллекции) разбиты на крупные блоки, которые называются партициями. В графе вычисления, который называется в Spark DAG (Direct Acyclic Graph), есть три основных компонента - `job`, `stage`, `task`.\n",
    "\n",
    "`job` представляет собой весь граф целиком, от момента создания DF, до применения `action` к нему. Состоит из одной или более `stage`. Когда возникает необходимость сделать `shuffle` данных, Spark создает новый `stage`. Каждый `stage` состоит из большого количества `task`. `task` это базовая операция над данными. Одновременно Spark выполняет N `task`, которые обрабатывают N партиций, где N - это суммарное число доступных потоков на всех воркерах.\n",
    "\n",
    "Исходя из этого, важно обеспечивать:\n",
    "+ достаточное количество партиций для распределения нагрузки по всем воркерам\n",
    "+ равномерное распределение данных между партициями\n",
    "\n",
    "Создадим датасет с перекосом данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit, col\n",
    "\n",
    "skew_column = when(col(\"id\") < 900, lit(0)).otherwise(lit(1)).alias(\"skew_column\")\n",
    "\n",
    "skewed_df = spark.range(1000).withColumn(\"skew\", skew_column).repartition(10, col(\"skew\"))\n",
    "\n",
    "skewed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_parts(df):\n",
    "    ret = df.rdd.mapPartitions(lambda x: [len(list(x))]).collect()\n",
    "    print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_parts(skewed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Любые операции с таким датасетом будут работать медленно, т.к.\n",
    "+ если суммарное количество потоков на всех воркерах больше 10, то в один момент времени работать будут максимум 10, остальные будут простаивать\n",
    "+ из 10 партицийи только в 2 есть данные и это означает, что только 2 потока будут обрабатывать данные, при этом из-за перекоса данных между ними (900 vs 100) первый станет bottleneck'ом\n",
    "\n",
    "Обычно перекошенные датасеты возникают после вычисления агрегатов, оконных функций и соединений, но также могут возникать и при чтении источников.\n",
    "\n",
    "Для устранения проблемы перекоса данных, следует использовать метод `repartition`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# здесь мы передаем только новое количество партиций и Spark выполнит RoundRobinPartitioning\n",
    "\n",
    "balanced_df = skewed_df.repartition(20)\n",
    "print_parts(balanced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# здесь мы добавляем к числу партиций колонку, по которой необходимо сделать репартиционирование,\n",
    "# поэтому Spark выполнит HashPartitioning\n",
    "\n",
    "balanced_df = skewed_df.repartition(20, col(\"id\"))\n",
    "print_parts(balanced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавление соли\n",
    "Часто при вычислении агрегатов приходится работать с перекошенными данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").options(header=True, inferSchema=True).load(\"/tmp/datasets/airport-codes.csv\")\n",
    "df.groupBy(col(\"type\")).count().orderBy(col(\"count\").desc()).show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, col\n",
    "\n",
    "skew_grouped = df.groupBy(col(\"type\")).agg(collect_list(col(\"ident\")).alias(\"ids\"))\n",
    "skew_grouped.show(20, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку при вычислении агрегата происходит неявный `HashPartitioning` по ключу (ключам) агрегата, то при выполнении определенных условий происходит нехватка памяти на воркере, которую нельзя исправить, не изменив подход к построению агрегата.\n",
    "\n",
    "Один из вариантов устранение - соление ключей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "salt = expr(\"\"\"pmod(round(rand() * 100, 0), 10)\"\"\").cast(\"integer\")\n",
    "salted = df.withColumn(\"salt\", salt)\n",
    "salted.select(col(\"type\"), col(\"ident\"), col(\"salt\")).sample(0.1).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это позволяет нам существенно снизить объем данных в каждой партиции (30к vs 3к):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "salted.groupBy(col(\"type\"), col(\"salt\")).count().orderBy(col(\"count\").desc()).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это позволяет нам посчитать требуемый агрегат более оптимальным путем, не смотря на появление второго агрегата:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salted \\\n",
    "    .groupBy(col(\"type\"), col(\"salt\")).agg(collect_list(col(\"ident\")).alias(\"ids\")) \\\n",
    "    .groupBy(col(\"type\")).agg(collect_list(col(\"ids\")).alias(\"ids\")) \\\n",
    "    .select(col(\"type\"), expr(\"\"\"flatten(ids)\"\"\").alias(\"ids\")) \\\n",
    "    .show(20, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ DF API позволяет строить большое количество агрегатов. При этом необходимо помнить, что операции `groupBy`, `cube`, `rollup` возвращают [org.apache.spark.sql.RelationalGroupedDataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset), к которому затем необходимо применить одну из функций агрегации - `count`, `sum`, `agg` и т. п.\n",
    "+ При вычислении агрегатов необходимо помнить, что эта операция требует перемешивания данных между воркерами, что, в случае перекошенных данных, может привести к OOM на воркере.\n",
    "\n",
    "## Кеширование\n",
    "По умолчанию при применении каждого действия Spark пересчитывает весь граф, что может негативно сказать на производительности приложения. Для демонстрации возьмем датасет [Airport Codes](https://datahub.io/core/airport-codes)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем несколько действий. Несмотря на то, что `only_ru` является общим для всех действий, он пересчитывается при вызове каждого действия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_ru = df.filter((col(\"iso_country\") == \"RU\") & (col(\"elevation_ft\") > 1000))\n",
    "only_ru.show(1, 50, True)\n",
    "\n",
    "only_ru.count()\n",
    "only_ru.collect()\n",
    "only_ru.groupBy(col(\"municipality\")).count().orderBy(col(\"count\").desc()).na.drop(\"any\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения этой проблемы следует использовать методы `cache`, либо `persist`. Данные методы сохраняют состояние графа после первого действия, и следующие обращаются к нему. Разница между методами заключается в том, что `persist` позволяет выбрать, куда сохранить данные, а `cache` использует значение по умолчанию. В текущей версии Spark это [StorageLevel.MEMORY_ONLY](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence). Важно помнить, что данный кеш не предназначен для обмена данными между разными Spark приложения - он является внутренним для приложения. После того, как работа с данными окончена, необходимо выполнить `unpersist` для очистки памяти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_ru = df.filter((col(\"iso_country\") == \"RU\") & (col(\"elevation_ft\") > 1000))\n",
    "only_ru.cache()\n",
    "only_ru.show(1, 50, True)\n",
    "only_ru.count()\n",
    "only_ru.collect()\n",
    "only_ru.groupBy(col(\"municipality\")).count().orderBy(col(\"count\").desc()).na.drop(\"any\").show()\n",
    "only_ru.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ Использование `cache` и `persist` позволяет существенно сократить время обработки данных, однако следует помнить и об увеличении потребляемой памяти на воркерах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Планы выполнения задач\n",
    "\n",
    "Любой `job` в Spark SQL имеет под собой план выполнения, кототорый генерируется на основе написанно запроса. План запроса содержит операторы, которые затем превращаются в Java код. Поскольку одну и ту же задачу в Spark SQL можно выполнить по-разному, полезно смотреть в планы выполнения, чтобы, например:\n",
    "+ убрать лишние shuffle\n",
    "+ убедиться, чтот тот или иной оператор будет выполнен на уровне источника, а не внутри Spark\n",
    "+ понять, как будет выполнен `join`\n",
    "\n",
    "Планы выполнения доступны в двух видах:\n",
    "+ метод `explain()` у DF\n",
    "+ на вкладке SQL в Spark UI\n",
    "\n",
    "Прочитаем датасет [Airport Codes](https://datahub.io/core/airport-codes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").options(header=True, inferSchema=True).load(\"/tmp/datasets/airport-codes.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем метод `explain`, чтобы посмотреть план запроса. Наиболее интересным является физический план, т.к. он отражает фактически алгоритм обработки данных. В данном случае в плане присутствует единственный оператор `FileScan csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним `filter` и проверим план выполнения. Читать план нужно снизу вверх. В плане появился новый оператор `filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(col(\"type\") == \"small_airport\").explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним агрегацию и проверим план выполнения. В нем появляется три оператора: 2 `HashAggregate` и `Exchange hashpartitioning`.\n",
    "\n",
    "Первый `HashAggregate` содержит функцию `partial_count(1)`. Это означает, что внутри каждого воркера произойдет подсчет строк по каждому ключу. Затем происходит `shuffle` по ключу агрегата, после которого выполняется еще один `HashAggregate` с функцией `count(1)`. Использование двух `HashAggregate` позволяет сократить количество передаваемых данных по сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(col(\"type\") == \"small_airport\").groupBy(col(\"iso_country\")).count().explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://cs5.pikabu.ru/post_img/big/2015/12/11/7/1449830295198229367.jpg\">\n",
    "\n",
    "### Выводы:\n",
    "+ Spark составляет физический план выполнения запроса на основании написанного вами кода\n",
    "+ Изучив план запроса, можно понять, какие операторы будут применены в ходе обработки ваших данных\n",
    "+ План выполнения запроса - один из основных инструментов оптимизации запроса\n",
    "\n",
    "## Оптимизация соединений и группировок\n",
    "При выполнении `join` двух DF важно следовать рекомендациям:\n",
    "+ фильтровать данные до join'а\n",
    "+ использовать equ join \n",
    "+ если можно путем увеличения количества данных применить equ join вместо non-equ join'а, то делать именно так\n",
    "+ всеми силами избегать cross-join'ов\n",
    "+ если правый DF помещается в памяти worker'а, использовать broadcast()\n",
    "\n",
    "### Виды соединений\n",
    "+ **BroadcastHashJoin**\n",
    "  - equ join\n",
    "  - broadcast\n",
    "+ **SortMergeJoin**\n",
    "  - equ join\n",
    "  - sortable keys\n",
    "+ **BroadcastNestedLoopJoin**\n",
    "  - non-equ join\n",
    "  - using broadcast\n",
    "+ **CartesianProduct**\n",
    "  - non-equ join\n",
    "  \n",
    "[Optimizing Apache Spark SQL Joins: Spark Summit East talk by Vida Ha](https://youtu.be/fp53QhSfQcI)\n",
    "\n",
    "Подготовим два датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = df.select(col(\"type\"), col(\"ident\"), col(\"iso_country\")).alias(\"left\").localCheckpoint()\n",
    "right = df.groupBy(col(\"type\")).count().alias(\"right\").localCheckpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BroadcastHashJoin\n",
    "+ работает, когда условие - равенство одного или нескольких ключей\n",
    "+ работает, когда один из датасетов небольшой и полностью вмещается в память воркера\n",
    "+ оставляет левый датасет как есть\n",
    "+ копирует правый датасет на каждый воркер\n",
    "+ составляет hash map из правого датасета, где ключ - кортеж из колонок в условии соединения\n",
    "+ итерируется по левому датасета внутри каждой партиции и проверяет наличие ключей в HashMap\n",
    "+ может быть автоматически использован, либо явно через `broadcast(df)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "result = left.join(broadcast(right), \"type\", \"inner\")\n",
    "\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SortMergeJoin\n",
    "+ работает, когда ключи соединения в обоих датасета являются сортируемыми\n",
    "+ репартиционирует оба датасета в 200 партиций по ключу (ключам) соединения\n",
    "+ сортирует партиции каждого из датасетов по ключу (ключам) соединения\n",
    "+ Используя сравнение левого и правого ключей, обходит каждую пару партиций и соединяет строки с одинаковыми ключами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "result = left.join(right, \"type\", \"inner\")\n",
    "\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BroadcastNestedLoopJoin\n",
    "+ работает, когда один из датасетов небольшой и полностью вмещается в память воркера\n",
    "+ оставляет левый датасет как есть\n",
    "+ копирует правый датасет на каждый воркер\n",
    "+ проходится вложенным циклом по каждой партиции левого датасета и копией правого датасета и проверяет условие\n",
    "+ может быть автоматически использован, либо явно через `broadcast(df)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "result = left.join(broadcast(right), left[\"type\"] != right[\"type\"], \"inner\")\n",
    "\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartesianProduct\n",
    "+ Создает пары из каждой партиции левого датасета с каждой партицией правого датасета, релоцирует каждую пару на один воркер и проверяет условие соединения\n",
    "+ на выходе создает N*M партиций\n",
    "+ работает медленнее остальных и часто приводит к ООМ воркеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "result = left.join(right, left[\"type\"] != right[\"type\"], \"inner\")\n",
    "\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Снижение количества shuffle\n",
    "В ряде случаев можно уйти от лишних `shuffle` операций при выполнении соединения. Для этого оба DF должны иметь одинаковое партиционирование - одинаковое количество партиций и ключ партиционирования, совпадающий с ключом соединения.\n",
    "\n",
    "Разница между планами выполнения будет хорошо видна в Spark UI на графе выполнения в Jobs и плане выполнения в SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.24 ms, sys: 0 ns, total: 7.24 ms\n",
      "Wall time: 2.26 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57421"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "left = df\n",
    "right = df.groupBy(col(\"type\")).count()\n",
    "joined = left.join(right, \"type\")\n",
    "joined.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.11 ms, sys: 0 ns, total: 8.11 ms\n",
      "Wall time: 1.88 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57421"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_rep = df.repartition(200, col(\"type\"))\n",
    "left = df_rep\n",
    "right = df_rep.groupBy(col(\"type\")).count()\n",
    "joined = left.join(right, \"type\")\n",
    "joined.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ В Spark используются 4 вида соединений: `BroadcastHashJoin`, `SortMergeJoin`, `BroadcastNestedLoopJoin`, `CartesianProduct`\n",
    "+ Выбор алгоритма основывается на условии соединения и размере датасетов\n",
    "+ `CartesianProduct` обладает самой низкой вычислительной эффективностью и его по возможности стоит избегать\n",
    "\n",
    "## Управление схемой данных\n",
    "В DF API каждая колонка имеет свой тип. Он может быть:\n",
    "+ скаляром - `StringType`, `IntegerType` и т. д.\n",
    "+ массивом - `ArrayType(T)`\n",
    "+ словарем `MapType(K, V)`\n",
    "+ структурой - `StructType()`\n",
    "\n",
    "DF целиком также имеет схему, описанную с помощью класса `StructType`\n",
    "\n",
    "Посмотреть список колонок можно с помощью атрибута `columns`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ident',\n",
       " 'type',\n",
       " 'name',\n",
       " 'elevation_ft',\n",
       " 'continent',\n",
       " 'iso_country',\n",
       " 'iso_region',\n",
       " 'municipality',\n",
       " 'gps_code',\n",
       " 'iata_code',\n",
       " 'local_code',\n",
       " 'coordinates']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схема DF доступна через атрибут `schema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_country:string,iso_region:string,municipality:string,gps_code:string,iata_code:string,local_code:string,coordinates:string>'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = df.schema\n",
    "schema.simpleString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructField(type,StringType,true)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema[\"type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'string'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = df.schema[\"type\"]\n",
    "foo.dataType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если указать схему при чтении источника, то spark не будет пытаться определить ее автоматически, что, в случае работы с такими типами файлов, как `csv` и `json`, сократит время создания `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "        .schema(schema) \\\n",
    "        .options(header=True, inferSchema=False) \\\n",
    "        .load(\"/tmp/datasets/airport-codes.csv\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схема может быть создана вручную:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType\n",
    "\n",
    "my_schema = \\\n",
    "    StructType(\n",
    "        [\n",
    "            StructField(\"foo\", StringType()),\n",
    "            StructField(\"bar\", StringType()),\n",
    "            StructField(\n",
    "                        \"boo\", \n",
    "                        StructType(\n",
    "                            [\n",
    "                                StructField(\"x\", IntegerType()),\n",
    "                                StructField(\"y\", BooleanType())\n",
    "                            ]\n",
    "                            )\n",
    "                       )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "my_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ Spark использует схемы для описания типов колонок, схемы всего DF, чтения источников и для работы с JSON\n",
    "+ Схема представляет собой инстанс класса `StructType`\n",
    "+ Колонки в Spark могут иметь любой тип. При этом вложенность словарей, массивов и структур не ограничена\n",
    "\n",
    "## Оптимизатор запросов Catalyst\n",
    "Catalyst выполняет оптимизацию запросов с целью ускорения их выполнения и применяет следующие методы:\n",
    " + Column projection\n",
    " + Partition pruning\n",
    " + Predicate pushdown\n",
    " + Constant folding\n",
    " \n",
    " Подготовим датасет для демонстрации работы Catalyst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ident: string, type: string, name: string, elevation_ft: int, continent: string, iso_region: string, municipality: string, gps_code: string, iata_code: string, local_code: string, coordinates: string, iso_country: string]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df \\\n",
    "    .write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .partitionBy(\"iso_country\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/tmp/airports.parquet\") \\\n",
    "\n",
    "airports = spark.read.parquet(\"/tmp/airports.parquet\")\n",
    "airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 244 items\n",
      "-rw-r--r--   3 aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/_SUCCESS\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=AD\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=AE\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=AF\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=AG\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=AI\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=AL\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=AM\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=AO\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=AQ\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=AR\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=AS\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=AT\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=AU\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=AW\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=AZ\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=BA\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=BB\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=BD\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=BE\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=BF\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=BG\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=BH\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=BI\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=BJ\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=BL\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=BM\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=BN\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=BO\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=BQ\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=BR\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=BS\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=BT\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=BW\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=BY\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=BZ\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=CA\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=CC\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=CD\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=CF\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=CG\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=CH\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=CI\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=CK\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=CL\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=CM\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=CN\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=CO\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=CR\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=CU\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=CV\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=CW\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=CX\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=CY\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=CZ\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=DE\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=DJ\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=DK\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=DM\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=DO\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=DZ\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=EC\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=EE\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=EG\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=EH\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=ER\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=ES\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=ET\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=FI\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=FJ\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=FK\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=FM\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=FO\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=FR\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=GA\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=GB\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=GD\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=GE\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=GF\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=GG\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=GH\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=GI\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=GL\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=GM\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=GN\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=GP\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=GQ\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=GR\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=GT\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=GU\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=GW\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=GY\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=HK\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=HN\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=HR\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=HT\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=HU\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=ID\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=IE\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=IL\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=IM\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=IN\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=IO\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=IQ\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=IR\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=IS\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=IT\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=JE\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=JM\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=JO\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=JP\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=KE\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=KG\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=KH\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=KI\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=KM\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=KN\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=KP\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=KR\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=KW\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=KY\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=KZ\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=LA\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=LB\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=LC\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=LI\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=LK\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=LR\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=LS\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=LT\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=LU\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=LV\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=LY\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MA\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MC\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MD\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=ME\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MF\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MG\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MH\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MK\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=ML\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MM\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MN\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MO\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MP\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MQ\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MR\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MS\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MT\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MU\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MV\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MW\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MX\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MY\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=MZ\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=NA\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=NC\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=NE\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=NF\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=NG\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=NI\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=NL\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=NO\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=NP\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=NR\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=NU\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=NZ\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=OM\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=PA\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=PE\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=PF\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=PG\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=PH\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=PK\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=PL\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=PM\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=PR\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=PS\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=PT\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=PW\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=PY\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=QA\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=RE\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=RO\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=RS\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=RU\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=RW\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=SA\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=SB\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=SC\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=SD\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=SE\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=SG\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=SH\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=SI\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=SK\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=SL\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=SM\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=SN\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=SO\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=SR\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=SS\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=ST\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=SV\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=SX\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=SY\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=SZ\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=TC\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=TD\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=TF\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=TG\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=TH\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=TJ\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=TL\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=TM\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=TN\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=TO\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=TR\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=TT\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=TV\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=TW\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=TZ\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=UA\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=UG\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=UM\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=US\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=UY\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=UZ\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=VA\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=VC\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=VE\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=VG\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=VI\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=VN\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=VU\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=WF\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=WS\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=XK\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=YE\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=YT\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=ZA\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=ZM\n",
      "drwxr-xr-x   - aadral hdfs          0 2021-04-18 19:26 /tmp/airports.parquet/iso_country=ZW\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/airports.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column projection\n",
    "Данный механизм позволяет избегать вычитывания ненужных колонок при работе с источниками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('ident, None)]\n",
      "+- Relation[ident#1140,type#1141,name#1142,elevation_ft#1143,continent#1144,iso_region#1145,municipality#1146,gps_code#1147,iata_code#1148,local_code#1149,coordinates#1150,iso_country#1151] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ident: string\n",
      "Project [ident#1140]\n",
      "+- Relation[ident#1140,type#1141,name#1142,elevation_ft#1143,continent#1144,iso_region#1145,municipality#1146,gps_code#1147,iata_code#1148,local_code#1149,coordinates#1150,iso_country#1151] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [ident#1140]\n",
      "+- Relation[ident#1140,type#1141,name#1142,elevation_ft#1143,continent#1144,iso_region#1145,municipality#1146,gps_code#1147,iata_code#1148,local_code#1149,coordinates#1150,iso_country#1151] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [ident#1140]\n",
      "+- *(1) FileScan parquet [ident#1140,iso_country#1151] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://brain-master.bigdatateam.org:8020/tmp/airports.parquet], PartitionCount: 243, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ident:string>\n",
      "CPU times: user 7.06 ms, sys: 0 ns, total: 7.06 ms\n",
      "Wall time: 3.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "selected = airports.select(col(\"ident\"))\n",
    "selected.cache()\n",
    "selected.count()\n",
    "selected.unpersist()\n",
    "selected.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Relation[ident#1140,type#1141,name#1142,elevation_ft#1143,continent#1144,iso_region#1145,municipality#1146,gps_code#1147,iata_code#1148,local_code#1149,coordinates#1150,iso_country#1151] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ident: string, type: string, name: string, elevation_ft: int, continent: string, iso_region: string, municipality: string, gps_code: string, iata_code: string, local_code: string, coordinates: string, iso_country: string\n",
      "Relation[ident#1140,type#1141,name#1142,elevation_ft#1143,continent#1144,iso_region#1145,municipality#1146,gps_code#1147,iata_code#1148,local_code#1149,coordinates#1150,iso_country#1151] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation[ident#1140,type#1141,name#1142,elevation_ft#1143,continent#1144,iso_region#1145,municipality#1146,gps_code#1147,iata_code#1148,local_code#1149,coordinates#1150,iso_country#1151] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) FileScan parquet [ident#1140,type#1141,name#1142,elevation_ft#1143,continent#1144,iso_region#1145,municipality#1146,gps_code#1147,iata_code#1148,local_code#1149,coordinates#1150,iso_country#1151] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://brain-master.bigdatateam.org:8020/tmp/airports.parquet], PartitionCount: 243, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_region:string,m...\n",
      "CPU times: user 4.6 ms, sys: 2.7 ms, total: 7.3 ms\n",
      "Wall time: 5.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "selected = airports\n",
    "selected.cache()\n",
    "selected.count()\n",
    "selected.unpersist()\n",
    "selected.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition pruning\n",
    "Данный механизм позволяет избежать чтения ненужных партиций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('iso_country = RU)\n",
      "+- Relation[ident#1140,type#1141,name#1142,elevation_ft#1143,continent#1144,iso_region#1145,municipality#1146,gps_code#1147,iata_code#1148,local_code#1149,coordinates#1150,iso_country#1151] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ident: string, type: string, name: string, elevation_ft: int, continent: string, iso_region: string, municipality: string, gps_code: string, iata_code: string, local_code: string, coordinates: string, iso_country: string\n",
      "Filter (iso_country#1151 = RU)\n",
      "+- Relation[ident#1140,type#1141,name#1142,elevation_ft#1143,continent#1144,iso_region#1145,municipality#1146,gps_code#1147,iata_code#1148,local_code#1149,coordinates#1150,iso_country#1151] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(iso_country#1151) && (iso_country#1151 = RU))\n",
      "+- Relation[ident#1140,type#1141,name#1142,elevation_ft#1143,continent#1144,iso_region#1145,municipality#1146,gps_code#1147,iata_code#1148,local_code#1149,coordinates#1150,iso_country#1151] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) FileScan parquet [ident#1140,type#1141,name#1142,elevation_ft#1143,continent#1144,iso_region#1145,municipality#1146,gps_code#1147,iata_code#1148,local_code#1149,coordinates#1150,iso_country#1151] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://brain-master.bigdatateam.org:8020/tmp/airports.parquet], PartitionCount: 1, PartitionFilters: [isnotnull(iso_country#1151), (iso_country#1151 = RU)], PushedFilters: [], ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_region:string,m...\n",
      "CPU times: user 0 ns, sys: 7.6 ms, total: 7.6 ms\n",
      "Wall time: 971 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filtered = airports.filter(col(\"iso_country\") == \"RU\")\n",
    "filtered.count()\n",
    "filtered.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicate pushdown\n",
    "Данный механизм позволяет \"протолкнуть\" условия фильтрации данных на уровень datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('iso_region = RU)\n",
      "+- Relation[ident#1140,type#1141,name#1142,elevation_ft#1143,continent#1144,iso_region#1145,municipality#1146,gps_code#1147,iata_code#1148,local_code#1149,coordinates#1150,iso_country#1151] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ident: string, type: string, name: string, elevation_ft: int, continent: string, iso_region: string, municipality: string, gps_code: string, iata_code: string, local_code: string, coordinates: string, iso_country: string\n",
      "Filter (iso_region#1145 = RU)\n",
      "+- Relation[ident#1140,type#1141,name#1142,elevation_ft#1143,continent#1144,iso_region#1145,municipality#1146,gps_code#1147,iata_code#1148,local_code#1149,coordinates#1150,iso_country#1151] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(iso_region#1145) && (iso_region#1145 = RU))\n",
      "+- Relation[ident#1140,type#1141,name#1142,elevation_ft#1143,continent#1144,iso_region#1145,municipality#1146,gps_code#1147,iata_code#1148,local_code#1149,coordinates#1150,iso_country#1151] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [ident#1140, type#1141, name#1142, elevation_ft#1143, continent#1144, iso_region#1145, municipality#1146, gps_code#1147, iata_code#1148, local_code#1149, coordinates#1150, iso_country#1151]\n",
      "+- *(1) Filter (isnotnull(iso_region#1145) && (iso_region#1145 = RU))\n",
      "   +- *(1) FileScan parquet [ident#1140,type#1141,name#1142,elevation_ft#1143,continent#1144,iso_region#1145,municipality#1146,gps_code#1147,iata_code#1148,local_code#1149,coordinates#1150,iso_country#1151] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://brain-master.bigdatateam.org:8020/tmp/airports.parquet], PartitionCount: 243, PartitionFilters: [], PushedFilters: [IsNotNull(iso_region), EqualTo(iso_region,RU)], ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_region:string,m...\n",
      "CPU times: user 5.2 ms, sys: 45 µs, total: 5.24 ms\n",
      "Wall time: 2.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filtered = airports.filter(col(\"iso_region\") == \"RU\")\n",
    "filtered.count()\n",
    "filtered.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify casts\n",
    "Данный механизм убирает ненужные `cast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias(cast('id as bigint), None)]\n",
      "+- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint\n",
      "Project [cast(id#1352L as bigint) AS id#1354L]\n",
      "+- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Range (0, 10, step=1, splits=2)\n"
     ]
    }
   ],
   "source": [
    "result = spark.range(10).select(col(\"id\").cast(\"long\"))\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias(cast(unresolvedalias(cast('id as int), None) as bigint), None)]\n",
      "+- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint\n",
      "Project [cast(cast(id#1360L as int) as bigint) AS id#1363L]\n",
      "+- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [cast(cast(id#1360L as int) as bigint) AS id#1363L]\n",
      "+- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [cast(cast(id#1360L as int) as bigint) AS id#1363L]\n",
      "+- *(1) Range (0, 10, step=1, splits=2)\n"
     ]
    }
   ],
   "source": [
    "result = spark.range(10).select(col(\"id\").cast(\"int\").cast(\"long\"))\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant folding\n",
    "Данный механизм сокращает количество констант, используемых в физическом плане"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [(3 > 0) AS foo#1367]\n",
      "+- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "foo: boolean\n",
      "Project [(3 > 0) AS foo#1367]\n",
      "+- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [true AS foo#1367]\n",
      "+- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [true AS foo#1367]\n",
      "+- *(1) Range (0, 10, step=1, splits=2)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "result = spark.range(10).select((lit(3) >  lit(0)).alias(\"foo\"))\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [('id > 0) AS foo#1371]\n",
      "+- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "foo: boolean\n",
      "Project [(id#1369L > cast(0 as bigint)) AS foo#1371]\n",
      "+- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [(id#1369L > 0) AS foo#1371]\n",
      "+- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [(id#1369L > 0) AS foo#1371]\n",
      "+- *(1) Range (0, 10, step=1, splits=2)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "result = spark.range(10).select((col(\"id\") >  lit(0)).alias(\"foo\"))\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine filters\n",
    "Данный механизм объединяет фильтры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('id < 10)\n",
      "+- Filter NOT (id#1373L = cast(5 as bigint))\n",
      "   +- Filter (id#1373L > cast(0 as bigint))\n",
      "      +- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint\n",
      "Filter (id#1373L < cast(10 as bigint))\n",
      "+- Filter NOT (id#1373L = cast(5 as bigint))\n",
      "   +- Filter (id#1373L > cast(0 as bigint))\n",
      "      +- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (((id#1373L > 0) && NOT (id#1373L = 5)) && (id#1373L < 10))\n",
      "+- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (((id#1373L > 0) && NOT (id#1373L = 5)) && (id#1373L < 10))\n",
      "+- *(1) Range (0, 10, step=1, splits=2)\n"
     ]
    }
   ],
   "source": [
    "result = spark.range(10).filter(col(\"id\") > 0).filter(col(\"id\") != 5).filter(col(\"id\") < 10)\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
